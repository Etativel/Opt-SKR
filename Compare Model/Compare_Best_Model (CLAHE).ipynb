{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAHE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "img_size = 150\n",
    "def get_training_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_21248\\771166761.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(data)\n"
     ]
    }
   ],
   "source": [
    "train = get_training_data('G:/#Kuliah/Skripsi/Skripsi/Code/Coding_Skripsi/data/train_CLAHE')      \n",
    "test = get_training_data('G:/#Kuliah/Skripsi/Skripsi/Code/Coding_Skripsi/data/test_CLAHE')\n",
    "val = get_training_data('G:/#Kuliah/Skripsi/Skripsi/Code/Coding_Skripsi/data/val_CLAHE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in test:\n",
    "    x_test.append(feature)\n",
    "    y_test.append(label)\n",
    "    \n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "x_test = np.array(x_test) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize data for deep learning \n",
    "x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val = x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "x_test = x_test.reshape(-1, img_size, img_size, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_21248\\1269967733.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(augmented_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count after augmentation: 7069\n"
     ]
    }
   ],
   "source": [
    "def load_augmented_data(augmented_dir, label, img_size):\n",
    "    augmented_data = []\n",
    "    for img in os.listdir(augmented_dir):\n",
    "        try:\n",
    "            img_arr = cv2.imread(os.path.join(augmented_dir, img), cv2.IMREAD_GRAYSCALE)\n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "            augmented_data.append([resized_arr, label])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return np.array(augmented_data)\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "save_dir = 'G:/#Kuliah/Skripsi/Skripsi/Code/Coding_Skripsi/data/augmented_CLAHE/NORMAL'  # Directory to save augmented images\n",
    "label = 'NORMAL'  # Class label to augment\n",
    "img_size = 150  # Image size (adjust if different)\n",
    "additional_images_needed = 2690  # Number of additional images needed\n",
    "batch_size = 32  # Number of images to generate per original image\n",
    "\n",
    "augmented_normal_data = load_augmented_data(save_dir, label=1, img_size=img_size)\n",
    "\n",
    "# Convert to appropriate format and normalize\n",
    "x_augmented_normal = np.array([i[0] for i in augmented_normal_data]) / 255\n",
    "x_augmented_normal = x_augmented_normal.reshape(-1, img_size, img_size, 1)\n",
    "y_augmented_normal = np.array([i[1] for i in augmented_normal_data])\n",
    "\n",
    "# Append to existing training data\n",
    "x_train = np.concatenate((x_train, x_augmented_normal), axis=0)\n",
    "y_train = np.concatenate((y_train, y_augmented_normal), axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Count after augmentation\n",
    "print(\"Count after augmentation:\", len(x_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestParam = [38,61,97,4,9,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 150, 150, 38)      646       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 75, 75, 38)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 75, 75, 61)        187819    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 75, 75, 61)        0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 38, 38, 61)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 38, 38, 97)        1917205   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 38, 38, 97)        0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 19, 19, 97)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 35017)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               4482304   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,588,103\n",
      "Trainable params: 6,588,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(bestParam[0] , (bestParam[3],bestParam[3]) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1)))\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(bestParam[1] , (bestParam[4],bestParam[4]) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(bestParam[2] , (bestParam[5],bestParam[5]) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128 , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
    "model.compile(optimizer = \"adam\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_1/conv2d_5/Relu' defined at (most recent call last):\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_21248\\375995993.py\", line 2, in <module>\n      history = model.fit(x_train,y_train, batch_size = 64 ,epochs = 10 , validation_data = [x_val, y_val] ,callbacks = [learning_rate_reduction])\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_1/conv2d_5/Relu'\nOOM when allocating tensor with shape[64,61,39,39] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/conv2d_5/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7038]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlearning_rate_reduction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mg:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mg:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1/conv2d_5/Relu' defined at (most recent call last):\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_21248\\375995993.py\", line 2, in <module>\n      history = model.fit(x_train,y_train, batch_size = 64 ,epochs = 10 , validation_data = [x_val, y_val] ,callbacks = [learning_rate_reduction])\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"g:\\Anaconda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_1/conv2d_5/Relu'\nOOM when allocating tensor with shape[64,61,39,39] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/conv2d_5/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7038]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "history = model.fit(x_train,y_train, batch_size = 64 ,epochs = 10 , validation_data = [x_val, y_val] ,callbacks = [learning_rate_reduction])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "predictions[:15]\n",
    "\n",
    "\n",
    "\n",
    "binary_predictions = [1 if i > 0.5 else 0 for i in predictions]\n",
    "cm = confusion_matrix(y_test,binary_predictions)\n",
    "\n",
    "True_Positive = []\n",
    "True_Negative = []\n",
    "False_Positive = []\n",
    "False_negative = []\n",
    "acc = []\n",
    "error = []\n",
    "precision = []\n",
    "recall = []\n",
    "TP_rate = []\n",
    "FP_rate = []\n",
    "specificity = []\n",
    "wrong_prediction = []\n",
    "F1 = []\n",
    "\n",
    "TP = cm[0][0]  # True Positives\n",
    "TN = cm[1][1]  # True Negatives\n",
    "FP = cm[0][1]  # False Positives\n",
    "FN = cm[1][0]  # False Negatives\n",
    "classification_accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "classification_error = (FP + FN) / (TP + TN + FP + FN)\n",
    "precision_calc = TP / (TP + FP)\n",
    "recall_calc = TP / (TP + FN)\n",
    "true_positive_rate = TP / (TP + FN)\n",
    "false_positive_rate = FP / (FP + TN)\n",
    "specificity_calc = TN / (TN + FP)\n",
    "F1_score = 2 * (precision_calc * recall_calc) / (precision_calc + recall_calc)\n",
    "wrong_calc = FN + FP\n",
    "\n",
    "True_Positive.append(TP)\n",
    "True_Negative.append(TN)\n",
    "False_Positive.append(FP)\n",
    "False_negative.append(FN)\n",
    "acc.append(classification_accuracy)\n",
    "error.append(classification_error)\n",
    "precision.append(precision_calc)\n",
    "recall.append(recall_calc)\n",
    "TP_rate.append(true_positive_rate)\n",
    "FP_rate.append(false_positive_rate)\n",
    "specificity.append(specificity_calc)\n",
    "F1.append(F1_score)\n",
    "wrong_prediction.append(wrong_calc)\n",
    "\n",
    "df = {\n",
    "    'TP': True_Positive,\n",
    "    'TN': True_Negative,\n",
    "    'FP': False_Positive,\n",
    "    'FN': False_negative,\n",
    "    'acc': acc,\n",
    "    'error': error,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'TP_rate': TP_rate,\n",
    "    'FP_rate': FP_rate,\n",
    "    'specificity': specificity,\n",
    "    'F1_score': F1,\n",
    "    'wrong_prediction' : wrong_prediction\n",
    "}\n",
    "\n",
    "normal_trial = pd.read_csv('CLAHE_trial1.csv')\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "normal_trial = pd.read_csv('CLAHE_trial1.csv')\n",
    "\n",
    "# Concatenate df with normal_trial along the row axis\n",
    "combined_df = pd.concat([normal_trial, df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame back to the CSV file\n",
    "combined_df.to_csv('CLAHE_trial1.csv', index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

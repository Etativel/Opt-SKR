{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "img_size = 150\n",
    "def get_training_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_15224\\771166761.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(data)\n"
     ]
    }
   ],
   "source": [
    "train = get_training_data('G:/#Kuliah/Skripsi/Skripsi/Code/Coding_Skripsi/data/train')      \n",
    "test = get_training_data('G:/#Kuliah/Skripsi/Skripsi/Code/Coding_Skripsi/data/test')\n",
    "val = get_training_data('G:/#Kuliah/Skripsi/Skripsi/Code/Coding_Skripsi/data/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in test:\n",
    "    x_test.append(feature)\n",
    "    y_test.append(label)\n",
    "    \n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "x_test = np.array(x_test) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize data for deep learning \n",
    "x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val = x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "x_test = x_test.reshape(-1, img_size, img_size, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_15224\\2657222256.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(augmented_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count after augmentation: 7069\n"
     ]
    }
   ],
   "source": [
    "def load_augmented_data(augmented_dir, label, img_size):\n",
    "    augmented_data = []\n",
    "    for img in os.listdir(augmented_dir):\n",
    "        try:\n",
    "            img_arr = cv2.imread(os.path.join(augmented_dir, img), cv2.IMREAD_GRAYSCALE)\n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "            augmented_data.append([resized_arr, label])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return np.array(augmented_data)\n",
    "\n",
    "# Define parameters\n",
    "data_dir = '../data/train'  # Adjust this path to your dataset\n",
    "save_dir = 'G:/#Kuliah/Skripsi/Skripsi/Code/Coding_Skripsi/data/augmented_normal/NORMAL'  # Directory to save augmented images\n",
    "label = 'NORMAL'  # Class label to augment\n",
    "img_size = 150  # Image size (adjust if different)\n",
    "additional_images_needed = 2690  # Number of additional images needed\n",
    "batch_size = 32  # Number of images to generate per original image\n",
    "\n",
    "augmented_normal_data = load_augmented_data(save_dir, label=1, img_size=img_size)\n",
    "\n",
    "# Convert to appropriate format and normalize\n",
    "x_augmented_normal = np.array([i[0] for i in augmented_normal_data]) / 255\n",
    "x_augmented_normal = x_augmented_normal.reshape(-1, img_size, img_size, 1)\n",
    "y_augmented_normal = np.array([i[1] for i in augmented_normal_data])\n",
    "\n",
    "# Append to existing training data\n",
    "x_train = np.concatenate((x_train, x_augmented_normal), axis=0)\n",
    "y_train = np.concatenate((y_train, y_augmented_normal), axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Count after augmentation\n",
    "print(\"Count after augmentation:\", len(x_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestParam = [15,48,88,3,5,5]\n",
    "bestParam = [33,47,73,4,8,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 150, 33)      561       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 75, 75, 33)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 75, 75, 47)        99311     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 75, 75, 47)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 38, 38, 47)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 38, 38, 73)        1372473   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 38, 38, 73)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 19, 19, 73)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 26353)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               3373312   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,845,786\n",
      "Trainable params: 4,845,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(bestParam[0] , (bestParam[3],bestParam[3]) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1)))\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(bestParam[1] , (bestParam[4],bestParam[4]) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(bestParam[2] , (bestParam[5],bestParam[5]) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128 , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
    "model.compile(optimizer = \"adam\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the range to match the number of epochs in your history data\n",
    "# epochs = [i for i in range(len(history.history['accuracy']))]\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# train_acc = history.history['accuracy']\n",
    "# train_loss = history.history['loss']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# ax[0].plot(epochs, train_acc, 'go-', label='Training Accuracy')\n",
    "# ax[0].plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "# ax[0].set_title('Training & Validation Accuracy')\n",
    "# ax[0].legend()\n",
    "# ax[0].set_xlabel(\"Epochs\")\n",
    "# ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "# ax[1].plot(epochs, train_loss, 'g-o', label='Training Loss')\n",
    "# ax[1].plot(epochs, val_loss, 'r-o', label='Validation Loss')\n",
    "# ax[1].set_title('Training & Validation Loss')\n",
    "# ax[1].legend()\n",
    "# ax[1].set_xlabel(\"Epochs\")\n",
    "# ax[1].set_ylabel(\"Loss\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "111/111 [==============================] - 24s 137ms/step - loss: 0.5143 - accuracy: 0.7294 - val_loss: 0.1695 - val_accuracy: 0.9385 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 12s 109ms/step - loss: 0.1501 - accuracy: 0.9444 - val_loss: 0.1467 - val_accuracy: 0.9487 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 12s 110ms/step - loss: 0.1207 - accuracy: 0.9571 - val_loss: 0.0983 - val_accuracy: 0.9761 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 12s 110ms/step - loss: 0.0843 - accuracy: 0.9709 - val_loss: 0.0911 - val_accuracy: 0.9675 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9726\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "111/111 [==============================] - 12s 110ms/step - loss: 0.0729 - accuracy: 0.9726 - val_loss: 0.0834 - val_accuracy: 0.9761 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 12s 110ms/step - loss: 0.0531 - accuracy: 0.9810 - val_loss: 0.0796 - val_accuracy: 0.9744 - lr: 3.0000e-04\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 12s 109ms/step - loss: 0.0453 - accuracy: 0.9837 - val_loss: 0.0602 - val_accuracy: 0.9846 - lr: 3.0000e-04\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 12s 110ms/step - loss: 0.0404 - accuracy: 0.9839 - val_loss: 0.0756 - val_accuracy: 0.9795 - lr: 3.0000e-04\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9851\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "111/111 [==============================] - 12s 110ms/step - loss: 0.0364 - accuracy: 0.9851 - val_loss: 0.0619 - val_accuracy: 0.9829 - lr: 3.0000e-04\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 12s 110ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.0628 - val_accuracy: 0.9829 - lr: 9.0000e-05\n",
      "19/19 [==============================] - 2s 54ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "history = model.fit(x_train,y_train, batch_size = 64 ,epochs = 10 , validation_data = [x_val, y_val] ,callbacks = [learning_rate_reduction])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "predictions[:15]\n",
    "\n",
    "\n",
    "\n",
    "binary_predictions = [1 if i > 0.5 else 0 for i in predictions]\n",
    "cm = confusion_matrix(y_test,binary_predictions)\n",
    "\n",
    "True_Positive = []\n",
    "True_Negative = []\n",
    "False_Positive = []\n",
    "False_negative = []\n",
    "acc = []\n",
    "error = []\n",
    "precision = []\n",
    "recall = []\n",
    "TP_rate = []\n",
    "FP_rate = []\n",
    "specificity = []\n",
    "wrong_prediction = []\n",
    "F1 = []\n",
    "\n",
    "TP = cm[0][0]  # True Positives\n",
    "TN = cm[1][1]  # True Negatives\n",
    "FP = cm[0][1]  # False Positives\n",
    "FN = cm[1][0]  # False Negatives\n",
    "classification_accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "classification_error = (FP + FN) / (TP + TN + FP + FN)\n",
    "precision_calc = TP / (TP + FP)\n",
    "recall_calc = TP / (TP + FN)\n",
    "true_positive_rate = TP / (TP + FN)\n",
    "false_positive_rate = FP / (FP + TN)\n",
    "specificity_calc = TN / (TN + FP)\n",
    "F1_score = 2 * (precision_calc * recall_calc) / (precision_calc + recall_calc)\n",
    "wrong_calc = FN + FP\n",
    "\n",
    "True_Positive.append(TP)\n",
    "True_Negative.append(TN)\n",
    "False_Positive.append(FP)\n",
    "False_negative.append(FN)\n",
    "acc.append(classification_accuracy)\n",
    "error.append(classification_error)\n",
    "precision.append(precision_calc)\n",
    "recall.append(recall_calc)\n",
    "TP_rate.append(true_positive_rate)\n",
    "FP_rate.append(false_positive_rate)\n",
    "specificity.append(specificity_calc)\n",
    "F1.append(F1_score)\n",
    "wrong_prediction.append(wrong_calc)\n",
    "\n",
    "df = {\n",
    "    'TP': True_Positive,\n",
    "    'TN': True_Negative,\n",
    "    'FP': False_Positive,\n",
    "    'FN': False_negative,\n",
    "    'acc': acc,\n",
    "    'error': error,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'TP_rate': TP_rate,\n",
    "    'FP_rate': FP_rate,\n",
    "    'specificity': specificity,\n",
    "    'F1_score': F1,\n",
    "    'wrong_prediction' : wrong_prediction\n",
    "}\n",
    "\n",
    "normal_trial = pd.read_csv('normal_trial2.csv')\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "normal_trial = pd.read_csv('normal_trial2.csv')\n",
    "\n",
    "# Concatenate df with normal_trial along the row axis\n",
    "combined_df = pd.concat([normal_trial, df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame back to the CSV file\n",
    "combined_df.to_csv('normal_trial2.csv', index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
